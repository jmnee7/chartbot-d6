# ğŸ“Š d6 í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ

í•œêµ­ ì£¼ìš” ìŒì› í”Œë«í¼ì˜ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” Python ê¸°ë°˜ í¬ë¡¤ë§ ì‹œìŠ¤í…œ

## ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ì•„í‚¤í…ì²˜](#ì•„í‚¤í…ì²˜)
- [ì„¤ì¹˜ ë° ì‹¤í–‰](#ì„¤ì¹˜-ë°-ì‹¤í–‰)
- [í¬ë¡¤ëŸ¬ ìƒì„¸](#í¬ë¡¤ëŸ¬-ìƒì„¸)
- [ì„¤ì •](#ì„¤ì •)
- [ë°ì´í„° êµ¬ì¡°](#ë°ì´í„°-êµ¬ì¡°)
- [ê°œë°œ ê°€ì´ë“œ](#ê°œë°œ-ê°€ì´ë“œ)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

## ê°œìš”

d6 í¬ë¡¤ëŸ¬ëŠ” ë‹¤ìŒ í”Œë«í¼ì˜ ì‹¤ì‹œê°„ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤:

- ğŸˆ **ë©œë¡ ** - TOP100, HOT100, ì¼ê°„, ì£¼ê°„, ì›”ê°„
- ğŸ§ **ì§€ë‹ˆ** - TOP200, ì¼ê°„, ì£¼ê°„, ì›”ê°„
- ğŸ› **ë²…ìŠ¤** - ì‹¤ì‹œê°„, ì¼ê°„, ì£¼ê°„
- ğŸ’œ **ë°”ì´ë¸Œ** - Today Top100, êµ­ë‚´ ê¸‰ìƒìŠ¹
- ğŸŒŠ **í”Œë¡œ** (FLO) - TOP100
- ğŸ“º **ìœ íŠœë¸Œ** - ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ í†µê³„

## ì•„í‚¤í…ì²˜

### í´ë˜ìŠ¤ êµ¬ì¡°

```
BaseCrawler (ì¶”ìƒ í´ë˜ìŠ¤)
â”œâ”€â”€ MelonCrawler
â”œâ”€â”€ GenieCrawler
â”œâ”€â”€ BugsCrawler
â”œâ”€â”€ VibeCrawler
â”œâ”€â”€ FloCrawler
â””â”€â”€ YoutubeCrawler
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

1. **BaseCrawler** (`base_crawler.py`)
   - ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ì¶”ìƒ í´ë˜ìŠ¤
   - ê³µí†µ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
   - HTTP ìš”ì²­ ì²˜ë¦¬ ë° ì—ëŸ¬ í•¸ë“¤ë§

2. **í”Œë«í¼ë³„ í¬ë¡¤ëŸ¬**
   - ê° í”Œë«í¼ì˜ íŠ¹ì„±ì— ë§ì¶˜ íŒŒì‹± ë¡œì§
   - ë™ì  ì½˜í…ì¸  ì²˜ë¦¬ (í•„ìš”ì‹œ)
   - ì°¨íŠ¸ íƒ€ì…ë³„ URL ë§¤í•‘

3. **RankTracker** (`rank_tracker.py`)
   - 24ì‹œê°„ ìˆœìœ„ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
   - ìˆœìœ„ ë³€ë™(delta) ê³„ì‚°
   - ì°¨íŠ¸ì•„ì›ƒ ì²˜ë¦¬

4. **íƒ€ê²Ÿ í•„í„°ë§** (`target_songs.py`)
   - íŠ¹ì • ì•„í‹°ìŠ¤íŠ¸/ê³¡ í•„í„°ë§
   - ë‹¤ì–‘í•œ ê²€ìƒ‰ ëª¨ë“œ ì§€ì›

## ì„¤ì¹˜ ë° ì‹¤í–‰

### ìš”êµ¬ì‚¬í•­

- Python 3.8 ì´ìƒ
- pip íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €

### ì„¤ì¹˜

```bash
cd crawlers
pip install -r requirements.txt
```

### ì‹¤í–‰

#### ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰
```bash
python main.py
```

#### íŠ¹ì • í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
```bash
python test_melon.py
python test_genie.py
python test_bugs.py
python test_vibe.py
python test_flo.py
```

#### ì˜µì…˜
```bash
# íŠ¹ì • ì°¨íŠ¸ íƒ€ì…ë§Œ í¬ë¡¤ë§
python main.py --chart-type realtime

# ë””ë²„ê·¸ ëª¨ë“œ
python main.py --debug

# íƒ€ê²Ÿ í•„í„°ë§ ë¹„í™œì„±í™”
python main.py --no-filter
```

## í¬ë¡¤ëŸ¬ ìƒì„¸

### MelonCrawler

**ì§€ì› ì°¨íŠ¸** (ì‹¤ì œ ì½”ë“œ ê¸°ì¤€)
- `top_100`: TOP 100 (ì‹¤ì‹œê°„)
- `hot_100`: HOT 100 (24ì‹œê°„)
- `daily`: ì¼ê°„ ì°¨íŠ¸
- `weekly`: ì£¼ê°„ ì°¨íŠ¸
- `monthly`: ì›”ê°„ ì°¨íŠ¸
- `realtime`: ì‹¤ì‹œê°„ (ê¸°ë³¸ê°’ê³¼ ë™ì¼)

**íŠ¹ì§•**
- User-Agent í—¤ë” í•„ìˆ˜
- ë™ì  ë¡œë”© ì½˜í…ì¸  ì²˜ë¦¬
- ì•¨ë²” ì´ë¯¸ì§€ URL ìˆ˜ì§‘

### GenieCrawler

**ì§€ì› ì°¨íŠ¸** (ì‹¤ì œ ì½”ë“œ ê¸°ì¤€)
- `top_100`: TOP 200 (ê¸°ë³¸ê°’, ì‹¤ì œë¡œëŠ” 200ê³¡)
- `realtime`: ì‹¤ì‹œê°„

**íŠ¹ì§•**
- JSON API í™œìš© ê°€ëŠ¥
- ìŠ¤íŠ¸ë¦¬ë° íšŸìˆ˜ ë°ì´í„° í¬í•¨
- ìƒì„¸ ì•„í‹°ìŠ¤íŠ¸ ì •ë³´ ì œê³µ

### BugsCrawler

**ì§€ì› ì°¨íŠ¸** (ì‹¤ì œ ì½”ë“œ ê¸°ì¤€)
- `top_100`: TOP 100 (ê¸°ë³¸ê°’)
- `realtime`: ì‹¤ì‹œê°„ ì°¨íŠ¸

**íŠ¹ì§•**
- ê°€ì¥ ë¹ ë¥¸ ì—…ë°ì´íŠ¸ ì£¼ê¸°
- ìŒì› ë¯¸ë¦¬ë“£ê¸° URL ì œê³µ
- ì¥ë¥´ë³„ ì°¨íŠ¸ ì§€ì›

### VibeCrawler

**ì§€ì› ì°¨íŠ¸** (ì‹¤ì œ ì½”ë“œ ê¸°ì¤€)
- `top_100`: Today Top 100 (ê¸°ë³¸ê°’)
- `realtime`: ì‹¤ì‹œê°„

**íŠ¹ì§•**
- ë„¤ì´ë²„ VIBE í”Œë«í¼
- íë ˆì´ì…˜ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì—°ë™
- ì•„í‹°ìŠ¤íŠ¸ ì±„ë„ ì •ë³´

### FloCrawler

**ì§€ì› ì°¨íŠ¸** (ì‹¤ì œ ì½”ë“œ ê¸°ì¤€)
- `top_100`: TOP 100 (ê¸°ë³¸ê°’)
- `realtime`: ì‹¤ì‹œê°„

**JSON ì¶œë ¥ í‚¤**: `flo`

**íŠ¹ì§•**
- SKí…”ë ˆì½¤ FLO í”Œë«í¼
- ê³ ìŒì§ˆ ìŠ¤íŠ¸ë¦¬ë° ì •ë³´
- ê°€ì‚¬ ë™ê¸°í™” ë°ì´í„°

### YoutubeCrawler

**ìˆ˜ì§‘ ë°ì´í„°**
- ì¡°íšŒìˆ˜ (views)
- ì¢‹ì•„ìš” ìˆ˜ (likes)
- ëŒ“ê¸€ ìˆ˜ (comments)
- ì—…ë¡œë“œ ë‚ ì§œ
- ì˜ìƒ ê¸¸ì´

**íŠ¹ì§•**
- YouTube Data API v3 ì‚¬ìš©
- API í‚¤ í•„ìš”
- í• ë‹¹ëŸ‰ ì œí•œ ê³ ë ¤

## ì„¤ì •

### íƒ€ê²Ÿ ì•„í‹°ìŠ¤íŠ¸/ê³¡ ì„¤ì •

`target_songs.py` íŒŒì¼ ìˆ˜ì •:

```python
# íƒ€ê²Ÿ ì•„í‹°ìŠ¤íŠ¸
TARGET_ARTIST = "DAY6"

# íƒ€ê²Ÿ ê³¡
TARGET_SONG = "HAPPY"

# íŠ¹ì • ì•„í‹°ìŠ¤íŠ¸ì˜ íŠ¹ì • ê³¡
TARGET_ARTIST_SONG = ("DAY6", "HAPPY")

# ê²€ìƒ‰ ëª¨ë“œ
SEARCH_MODE = "artists"  # "songs", "artist_songs", "all"
```

### ê²€ìƒ‰ ëª¨ë“œ

- **`artists`**: ì§€ì •ëœ ì•„í‹°ìŠ¤íŠ¸ì˜ ëª¨ë“  ê³¡
- **`songs`**: ì§€ì •ëœ ê³¡ëª…ë§Œ (ì•„í‹°ìŠ¤íŠ¸ ë¬´ê´€)
- **`artist_songs`**: íŠ¹ì • ì•„í‹°ìŠ¤íŠ¸ì˜ íŠ¹ì • ê³¡ë§Œ
- **`all`**: ìœ„ ëª¨ë“  ì¡°ê±´ OR ì—°ì‚°

### í™˜ê²½ ë³€ìˆ˜

```bash
# YouTube API í‚¤ (ì„ íƒì‚¬í•­)
export YOUTUBE_API_KEY="your-api-key"

# í”„ë¡ì‹œ ì„¤ì • (ì„ íƒì‚¬í•­)
export HTTP_PROXY="http://proxy.example.com:8080"
export HTTPS_PROXY="http://proxy.example.com:8080"
```

## ë°ì´í„° êµ¬ì¡°

### ì¶œë ¥ íŒŒì¼

```
docs/public-data/
â”œâ”€â”€ latest.json           # ìµœì‹  í†µí•© ë°ì´í„°
â”œâ”€â”€ summary.json          # ìš”ì•½ í†µê³„
â”œâ”€â”€ happy.json            # íŠ¹ì • ê³¡ ìƒì„¸
â”œâ”€â”€ day6_chart.json       # ì°¨íŠ¸ í˜ì´ì§€ ë°ì´í„°
â””â”€â”€ youtube_stats.json    # YouTube í†µê³„
```

### JSON ìŠ¤í‚¤ë§ˆ

#### ì°¨íŠ¸ ë°ì´í„°
```json
{
  "platform": "melon",
  "chartType": "top100",
  "collectedAt": "2025-10-10T15:00:00+09:00",
  "tracks": [
    {
      "rank": 1,
      "title": "HAPPY",
      "artist": "DAY6",
      "album": "Fourever",
      "imageUrl": "https://...",
      "delta": 2,
      "isNew": false,
      "timestamp": "2025101015"
    }
  ]
}
```

#### ìˆœìœ„ ë³€ë™ ë°ì´í„°
```json
{
  "songId": "DAY6_HAPPY",
  "history": [
    {
      "timestamp": "2025101015",
      "rank": 97
    },
    {
      "timestamp": "2025101014",
      "rank": 99
    }
  ],
  "delta": 2,
  "peak": 85,
  "chartInHours": 168
}
```

## ê°œë°œ ê°€ì´ë“œ

### ìƒˆ í¬ë¡¤ëŸ¬ ì¶”ê°€í•˜ê¸°

1. **BaseCrawler ìƒì†**
```python
from base_crawler import BaseCrawler

class NewPlatformCrawler(BaseCrawler):
    def __init__(self):
        super().__init__("newplatform")
```

2. **í•„ìˆ˜ ë©”ì„œë“œ êµ¬í˜„**
```python
def get_chart_url(self, chart_type):
    urls = {
        "realtime": "https://platform.com/chart/realtime",
        "daily": "https://platform.com/chart/daily"
    }
    return urls.get(chart_type)

def get_song_elements(self, soup, chart_type):
    return soup.select(".song-item")

def parse_song_data(self, element, chart_type):
    return {
        "rank": element.select_one(".rank").text,
        "title": element.select_one(".title").text,
        "artist": element.select_one(".artist").text,
        "album": element.select_one(".album").text
    }
```

3. **main.pyì— ë“±ë¡**
```python
from newplatform_crawler import NewPlatformCrawler

crawlers = [
    MelonCrawler(),
    GenieCrawler(),
    # ... 
    NewPlatformCrawler()  # ì¶”ê°€
]
```

### í…ŒìŠ¤íŠ¸ ì‘ì„±

```python
# test_newplatform.py
from newplatform_crawler import NewPlatformCrawler

def test_crawler():
    crawler = NewPlatformCrawler()
    data = crawler.get_chart("realtime")
    
    assert data is not None
    assert len(data) > 0
    assert "rank" in data[0]
    
    print(f"âœ“ ìˆ˜ì§‘ëœ ê³¡: {len(data)}ê°œ")
    print(f"âœ“ 1ìœ„: {data[0]['title']} - {data[0]['artist']}")

if __name__ == "__main__":
    test_crawler()
```

### ì—ëŸ¬ ì²˜ë¦¬

```python
class CustomCrawler(BaseCrawler):
    def get_chart(self, chart_type):
        try:
            # í¬ë¡¤ë§ ë¡œì§
            return data
        except ConnectionError:
            print(f"âš ï¸ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨: {self.platform_name}")
            return []
        except ParseError as e:
            print(f"âš ï¸ íŒŒì‹± ì—ëŸ¬: {e}")
            return []
        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
            return []
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. í¬ë¡¤ë§ ì‹¤íŒ¨
```
ë¬¸ì œ: ConnectionError ë˜ëŠ” TimeoutError
í•´ê²°:
- ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
- íƒ€ê²Ÿ ì‚¬ì´íŠ¸ ì ‘ì† ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
- User-Agent í—¤ë” ì—…ë°ì´íŠ¸
- í”„ë¡ì‹œ ì„¤ì • í™•ì¸
```

#### 2. íŒŒì‹± ì—ëŸ¬
```
ë¬¸ì œ: ë¹ˆ ë°ì´í„° ë˜ëŠ” AttributeError
í•´ê²°:
- ì‚¬ì´íŠ¸ HTML êµ¬ì¡° ë³€ê²½ í™•ì¸
- CSS ì„ íƒì ì—…ë°ì´íŠ¸
- ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€
```

#### 3. ì¸ì½”ë”© ë¬¸ì œ
```
ë¬¸ì œ: UnicodeDecodeError
í•´ê²°:
- response.encoding = 'utf-8' ì„¤ì •
- BeautifulSoupì— ì¸ì½”ë”© ëª…ì‹œ
- íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
```

#### 4. Rate Limiting
```
ë¬¸ì œ: 429 Too Many Requests
í•´ê²°:
- ìš”ì²­ ê°„ê²© ì¦ê°€ (time.sleep)
- í”„ë¡ì‹œ ë¡œí…Œì´ì…˜
- User-Agent ë¡œí…Œì´ì…˜
- ë¶„ì‚° í¬ë¡¤ë§
```

### ë””ë²„ê¹… íŒ

1. **ìƒì„¸ ë¡œê·¸ í™œì„±í™”**
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

2. **HTML ì €ì¥ í›„ ë¶„ì„**
```python
with open('debug.html', 'w', encoding='utf-8') as f:
    f.write(response.text)
```

3. **ì„ íƒì í…ŒìŠ¤íŠ¸**
```python
from bs4 import BeautifulSoup

html = open('debug.html', 'r', encoding='utf-8').read()
soup = BeautifulSoup(html, 'html.parser')

# ì„ íƒì í…ŒìŠ¤íŠ¸
elements = soup.select('.your-selector')
print(f"ì°¾ì€ ìš”ì†Œ: {len(elements)}ê°œ")
```

4. **ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§**
```bash
# HTTP íŠ¸ë˜í”½ í™•ì¸
tcpdump -i any -s 0 -A 'tcp port 80 or tcp port 443'

# DNS ì¿¼ë¦¬ í™•ì¸
tcpdump -i any -s 0 port 53
```

## ì„±ëŠ¥ ìµœì í™”

### ë³‘ë ¬ ì²˜ë¦¬
```python
from concurrent.futures import ThreadPoolExecutor

def crawl_all_platforms():
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []
        for crawler in crawlers:
            future = executor.submit(crawler.get_chart, "realtime")
            futures.append(future)
        
        results = [f.result() for f in futures]
    return results
```

### ìºì‹±
```python
from functools import lru_cache

@lru_cache(maxsize=128)
def get_cached_chart(platform, chart_type):
    crawler = get_crawler(platform)
    return crawler.get_chart(chart_type)
```

### ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
import gc

def process_large_dataset():
    data = fetch_data()
    process(data)
    
    # ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ
    del data
    gc.collect()
```

## ë¼ì´ì„ ìŠ¤

MIT License - ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](../LICENSE) íŒŒì¼ ì°¸ì¡°

---

Made with â¤ï¸ for DAY6 and My Days